{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wellbeing Dashboard Python\n",
    "## B - Imputing Missing Values\n",
    "This Jupyter Notebook imputes \"N/A\" or missing values using the sklearns experimental iterative imputer. Make sure you update your sklearn and make the appropriate installations for importing the imputer library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing all major library imports\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression,LogisticRegressionCV \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import scikitplot as skplt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data that was merged in the previous step (A - Merging Data)\n",
    "master = pd.read_csv ('../raw_data/poverty_data.csv')\n",
    "print (master.shape)\n",
    "master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = pd.DataFrame(master.dtypes, columns=['dtype'])\n",
    "dtypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure all the predictors are float\n",
    "print (dtypes[dtypes.dtype == object].shape)\n",
    "dtypes[dtypes.dtype == object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating Iterative Imputer (docs: https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer)\n",
    "# First need to import this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # Make sure your Sklearn is updated - https://stackoverflow.com/questions/58332191/modulenotfounderror-no-module-named-sklearn-experimental\n",
    "# Bow you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolating countries that do not have enough values\n",
    "vals_per_country = pd.DataFrame(master.groupby('country_code').count().sum(axis=1))\n",
    "vals_per_country.sort_values(by=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only including countries that have more than 15000 values\n",
    "countries_to_include = vals_per_country[vals_per_country[0] >= 15000].index\n",
    "print (len(countries_to_include))\n",
    "m2 = master[master.country_code.isin(countries_to_include)]\n",
    "print (len(m2.country_code.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing before and after country exclusion\n",
    "print (master.shape)\n",
    "print (m2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further trying to exclude those countries that don't have enough data\n",
    "avg_vals_per_country = pd.DataFrame(m2.groupby('country_code').count().mean())\n",
    "avg_vals_per_country.sort_values(by=0).head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any columns that have less than an average of 5 values per country\n",
    "predictors_to_include = avg_vals_per_country[avg_vals_per_country[0] >= 5.0].index[2:]\n",
    "predictors_to_include = pd.DataFrame(predictors_to_include, columns = ['ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all columns that have \"LCU\" or local currency units\n",
    "predictors_to_include = predictors_to_include[~predictors_to_include.ind.str.contains ('LCU')] \n",
    "predictors_to_include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing other superfluous columns\n",
    "remove_superfluous_cols = ['Agriculture, forestry, and fishing, value added (current US$)',\n",
    "                           'Changes in inventories (current US$)',\n",
    "                           'Changes in stocks (petajoules)',\n",
    "                           'Consumer price index: General',\n",
    "                           'Current health expenditure (% of GDP)_x',\n",
    "                           'Current health expenditure (% of GDP)_y',\n",
    "                           'Foreign direct investment, net inflows (% of GDP)_x',\n",
    "                           'GDP (current US$)'\n",
    "                          ]\n",
    "predictors_to_include = predictors_to_include[~predictors_to_include.ind.isin (remove_superfluous_cols)] \n",
    "list(predictors_to_include.ind.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating master list of columns for imputation\n",
    "new_columns = list(master.columns [0:3]) + list(predictors_to_include.ind.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new dataframe with just the list of new columns\n",
    "m3 = m2[new_columns]\n",
    "m3.reset_index(drop = True, inplace=True)\n",
    "m3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally Populating / Imputing Nulls!\n",
    "# Step 1 is to create all the functions that will be needed to run the imputer.\n",
    "# We will be extracting each country and imputing values for each country based on real values that are available for...\n",
    "# ...that country\n",
    "\n",
    "def extractor(df, country_code):\n",
    "    \"\"\"Extracting a country-specific dataset\"\"\"\n",
    "\n",
    "    extract = df[df.country_code == country_code]\n",
    "    extract.reset_index(inplace=True, drop = True)\n",
    "    extract = extract.iloc[:,3:]\n",
    "    extract.dropna(axis=1, inplace=True, how='any', thresh = 5) #removing columns that don't have atleast 5 values\n",
    "    return extract\n",
    "\n",
    "def imputer (c_extract, imputer_engine):\n",
    "    \"\"\"Imputing values for the extracted country\"\"\"\n",
    "    \n",
    "    imputed = pd.DataFrame(imputer_engine.fit_transform(c_extract))\n",
    "    imputed.columns = c_extract.columns\n",
    "    return imputed\n",
    "    \n",
    "def replacer (df, country_code, imputed):\n",
    "    \"\"\"Extracting country-specific dataset again to ensure that all columns are included\"\"\"\n",
    "    \n",
    "    extract_all_columns = df[df.country_code == country_code] \n",
    "    extract_all_columns.reset_index (inplace=True, drop = True)\n",
    "    for col in imputed:\n",
    "        extract_all_columns[col] = imputed[col].copy()\n",
    "    return extract_all_columns\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "def master_imputer (df, imputer_engine):\n",
    "    \"\"\"Master function that combines all the earlier functions to impute values for all countries and return one combined dataset\"\"\"\n",
    "    \n",
    "    df_imputed = pd.DataFrame()\n",
    "    \n",
    "    for country_code in tqdm_notebook(df.country_code.unique()):\n",
    "        c_extract = extractor (df, country_code)\n",
    "        imputed = imputer (c_extract, imputer_engine)\n",
    "        replaced = replacer (df, country_code, imputed)\n",
    "        df_imputed = pd.concat ([df_imputed, replaced], axis = 0)\n",
    "    \n",
    "    df_imputed.reset_index (inplace=True, drop = True)\n",
    "    \n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing all nulls\n",
    "\n",
    "# The model that we would be using for imputation\n",
    "model = LassoCV(normalize=True, cv=3, n_alphas=100)\n",
    "\n",
    "# The iterator we would be using with the relevant parameters\n",
    "ii = IterativeImputer (estimator = model, tol = 0.00000000001, n_nearest_features=100)\n",
    "\n",
    "# Calling the master_imputer function created earlier\n",
    "working = master_imputer (m3, ii)\n",
    "\n",
    "# Analyzing our results\n",
    "print (working.shape)\n",
    "print (working.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting imputed file\n",
    "working.to_csv('../raw_data/iterated.csv', index=False)\n",
    "\n",
    "# Exporting original file (with same columns)\n",
    "m3.to_csv ('../raw_data/wo_iterated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working.head(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
