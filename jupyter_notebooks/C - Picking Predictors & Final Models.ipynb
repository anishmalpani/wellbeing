{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wellbeing Dashboard Python\n",
    "## C - Picking Top Predictors & Building Final Models\n",
    "This Jupyter Notebook evaluates over 500 impact indicators and tries to isolate the top 19 indicators that are strong predictors of well-being and are as well \"controllable\". Controllable is defined using intuition and ensuring that the indicators are something that entrepreneurs could potentially impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing all major library imports\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression,LogisticRegressionCV \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor, BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import scikitplot as skplt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing files that were generated in the earlier step (B - Imputing Missing Values)\n",
    "\n",
    "witer = pd.read_csv ('../raw_data/iterated.csv') # with iterated / imputed values\n",
    "woiter = pd.read_csv ('../raw_data/wo_iterated.csv') # without iterated / imputed values\n",
    "print (witer.shape)\n",
    "print (woiter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the data\n",
    "witer.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Picking the top, most impactful indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping columns and rows based on thresholds that maximize the number of values we have avaible.\n",
    "# Despite imputing values, some countries and indicators just didn't have enough information to make intelligent imputations.\n",
    "# For this, I ran a gridsearch of sorts where I evaluated which thresholds save the most values\n",
    "work = witer.dropna(axis=1, thresh=2600).dropna(axis=0, thresh=735).dropna(axis=1)\n",
    "print (work.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To mitigate risk of imputed values, I need to ensure that my test set is as robust as possible.\n",
    "# To do that, I ensured that my test set is full of true values.\n",
    "true_values = woiter[work.columns]\n",
    "true_values = true_values[true_values.index.isin (work.index)]\n",
    "print (true_values.shape)\n",
    "print (true_values.isnull().sum().sum())\n",
    "true_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column called fill_level to evaluate how many true values each row has\n",
    "true_values['fill_level'] = true_values.count(axis=1) / len(true_values.columns)\n",
    "\n",
    "# Isolating the top 200 rows with the most true values / highest fill level\n",
    "true_values_index = true_values.sort_values (by = 'fill_level', ascending = False).head(200).index\n",
    "true_values = work[work.index.isin (true_values_index)]\n",
    "print (true_values.shape)\n",
    "true_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating those rows that have more iterated values than true values\n",
    "iterated_values = work[~work.index.isin (true_values.index)]\n",
    "print (iterated_values.shape)\n",
    "iterated_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing what the year breakout is for our true values set\n",
    "true_values.date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dev set which will consist partially of the true value data set and the iterated value data set\n",
    "dev_true = true_values[true_values.date.isin (['2005','2014'])]\n",
    "print (dev_true.shape)\n",
    "dev_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing what years are in the iterated value data set\n",
    "iterated_values.date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating some years to include in the dev set from the iterated values\n",
    "\n",
    "dev_iterated = iterated_values[iterated_values.date.isin (['2010','2013', '1993', '2004'])]\n",
    "print (dev_iterated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the dev_true dataset and the dev_iterated dataset to get our final dev dataset\n",
    "dev = pd.concat([dev_true,dev_iterated])\n",
    "print (dev.shape)\n",
    "dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the test set that consists largely of true values\n",
    "test = true_values[~true_values.index.isin (dev_true.index)]\n",
    "print (test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the train set that consists largely of iterated values. This isn't ideal but it is more important for the test set to be real.\n",
    "train = work[~(work.index.isin (test.index)) & ~(work.index.isin (dev.index))]\n",
    "print (train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the shape of all our working sets for Step 1 of this process of isolating top indicators\n",
    "print (work.shape)\n",
    "print (train.shape)\n",
    "print (dev.shape)\n",
    "print (test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating our target indicators\n",
    "\n",
    "target_indicators = [\n",
    "       'Mean years of schooling (years)',\n",
    "       'Gross national income (GNI) per capita (2011 PPP $)',\n",
    "       'Life expectancy at birth (years)',\n",
    "       'Expected years of schooling (years)', \n",
    "]\n",
    "\n",
    "target_indicators_all = [\n",
    "       'Mean years of schooling (years)',\n",
    "       'Life expectancy index', \n",
    "        'Income index',\n",
    "       'Education index', \n",
    "        'Human Development Index (HDI)',\n",
    "       'Gross national income (GNI) per capita (2011 PPP $)',\n",
    "       'Life expectancy at birth (years)',\n",
    "       'Expected years of schooling (years)', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictors to drop because they are too closely related or not controllable.\n",
    "# This was a manual and iterative process. Every time I ran a model, more of these indicators emerged as strong predictors...\n",
    "# ...but were not controllable or were too closely related to the component factors of the well-being index (HDI)\n",
    "\n",
    "predictors_to_drop = [\n",
    "'GDP per capita, PPP (constant 2011 international $)',\n",
    "'GDP per capita (2011 PPP $)',\n",
    "'Estimated gross national income per capita, male (2011 PPP $)',\n",
    "'GNI per capita, PPP (constant 2011 international $)',\n",
    "'Estimated gross national income per capita, female (2011 PPP $)',\n",
    "'GDP per person employed (constant 2011 PPP $)',\n",
    "'GNI per capita, PPP (current international $)',\n",
    "'Gross domestic product per capita, constant prices',\n",
    "'Primary income receipts (BoP, current US$)',\n",
    "'GDP per capita, PPP (current international $)',\n",
    "'GNI per capita (constant 2010 US$)',\n",
    "'GDP per capita (constant 2010 US$)',\n",
    "'GNI, PPP (current international $)',\n",
    "'GNI, PPP (constant 2011 international $)',\n",
    "'GNI, Atlas method (current US$)',\n",
    "'GNI per capita, Atlas method (current US$)',\n",
    "'GNI per capita growth (annual %)',\n",
    "'GNI growth (annual %)',\n",
    "'GNI (current US$)',\n",
    "'GNI (constant 2010 US$)',\n",
    "'GDP, PPP (current international $)',\n",
    "'GDP, PPP (constant 2011 international $)',\n",
    "'GDP per unit of energy use (constant 2011 PPP $ per kg of oil equivalent)',\n",
    "'Gross capital formation (annual % growth)',\n",
    "'Gross capital formation (constant 2010 US$)',\n",
    "'Gross capital formation (current US$)',\n",
    "'Gross domestic product (GDP), total (2011 PPP $ billions)',\n",
    "'Households and NPISHs Final consumption expenditure per capita (constant 2010 US$)',\n",
    "'GDP per capita (current US$)',\n",
    "'Price level ratio of PPP conversion factor (GDP) to market exchange rate',\n",
    "'Gross savings (% of GDP)',\n",
    "'Energy use (kg of oil equivalent) per $1,000 GDP (constant 2011 PPP)',\n",
    "'Energy intensity level of primary energy (MJ/$2011 PPP GDP)',\n",
    "'Electric power consumption (kWh per capita)',\n",
    "'CO2 emissions (kg per 2010 US$ of GDP)',\n",
    "'GDP per unit of energy use (PPP $ per kg of oil equivalent)',\n",
    "'CO2 emissions (kg per 2011 PPP $ of GDP)',\n",
    "'CO2 emissions (kg per PPP $ of GDP)',\n",
    "'Carbon dioxide emissions (kg per 2011 PPP $ of GDP)',\n",
    "'CO2 intensity (kg per kg of oil equivalent energy use)',\n",
    "'Out-of-pocket expenditure per capita (current US$)',\n",
    "'Domestic general government health expenditure (% of current health expenditure)',\n",
    "'Domestic general government health expenditure per capita (current US$)',\n",
    "'Carbon dioxide emissions, per capita (tonnes)',\n",
    "'Mortality rate, under-5 (per 1,000 live births)',\n",
    "'Mortality rate, female adult (per 1,000 people)',\n",
    "'Mortality rate, male adult (per 1,000 people)',\n",
    "'Mortality rate, infant (per 1,000 live births)_y',\n",
    "'Mortality rate, neonatal (per 1,000 live births)',\n",
    "'Mortality rate, under-five (per 1,000 live births)',\n",
    "'Mortality rate, infant (per 1,000 live births)_x',\n",
    "'Maternal mortality ratio (deaths per 100,000 live births)',\n",
    "'Population with at least some secondary education (% ages 25 and older)',\n",
    "'Population with at least some secondary education, female (% ages 25 and older)',\n",
    "'School enrollment, secondary (% gross)',\n",
    "'Population with at least some secondary education, male (% ages 25 and older)',\n",
    "'GDP deflator (base year varies by country)',\n",
    "'Current health expenditure per capita (current US$)',\n",
    "'Out-of-pocket expenditure (% of current health expenditure)',\n",
    "'Current health expenditure per capita, PPP (current international $)',\n",
    "'Population ages 15-64 (% of total population)',\n",
    "'Population ages 80 and above, male (% of male population)',\n",
    "'Population ages 55-59, male (% of male population)',\n",
    "'Current health expenditure per capita (current US$)',\n",
    "'GDP deflator: linked series (base year varies by country)',\n",
    "'PM2.5 air pollution, population exposed to levels exceeding WHO guideline value (% of total)',\n",
    "'Reserves and related items (BoP, current US$)',\n",
    "'Inflation, GDP deflator (annual %)',\n",
    "'Population ages 30-34, female (% of female population)',\n",
    "'Population ages 55-59, female (% of female population)',\n",
    "'Population ages 35-39, female (% of female population)',\n",
    "'Population, male (% of total population)',\n",
    "'Age dependency ratio, old (% of working-age population)',\n",
    "'Age dependency ratio, young (% of working-age population)',\n",
    "'Agriculture, forestry, and fishing, value added (annual % growth)',\n",
    "'Air transport, freight (million ton-km)',\n",
    "'Air transport, registered carrier departures worldwide',\n",
    "'Arable land (hectares)',\n",
    "'Broad money growth (annual %)',\n",
    "'CO2 emissions from electricity and heat production, total (% of total fuel combustion)',\n",
    "'CO2 emissions from liquid fuel consumption (% of total)',\n",
    "'CO2 emissions from liquid fuel consumption (kt)',\n",
    "'CO2 emissions from residential buildings and commercial and public services (% of total fuel combustion)',\n",
    "'CO2 emissions from transport (% of total fuel combustion)',\n",
    "'Coefficient of human inequality',\n",
    "'Current account balance',\n",
    "'Current account balance (BoP, current US$)',\n",
    "'Food production index (2004-2006 = 100)',\n",
    "'Foreign direct investment, net inflows (% of GDP)_y',\n",
    "'Population ages 00-04, female (% of female population)',\n",
    "'Population ages 00-04, male (% of male population)',\n",
    "'Population ages 0-14 (% of total population)',\n",
    "'Population ages 0-14, female',\n",
    "'Population ages 0-14, female (% of female population)',\n",
    "'Population ages 0-14, male',\n",
    "'Population ages 0-14, male (% of male population)',\n",
    "'Population ages 0-14, total',\n",
    "'Population ages 05-09, female (% of female population)',\n",
    "'Population ages 10-14, female (% of female population)',\n",
    "'Population ages 10-14, male (% of male population)',\n",
    "'Population ages 15-19, female (% of female population)',\n",
    "'Population ages 15-64, female',\n",
    "'Population ages 15-64, male',\n",
    "'Population ages 15-64, total',\n",
    "'Population ages 15â€“64 (millions)',\n",
    "'Population ages 25-29, female (% of female population)',\n",
    "'Population ages 25-29, male (% of male population)',\n",
    "'Population ages 30-34, male (% of male population)',\n",
    "'Population ages 35-39, male (% of male population)',\n",
    "'Population ages 40-44, female (% of female population)',\n",
    "'Population ages 40-44, male (% of male population)',\n",
    "'Population ages 45-49, female (% of female population)',\n",
    "'Population ages 45-49, male (% of male population)',\n",
    "'Population ages 50-54, female (% of female population)',\n",
    "'Population ages 50-54, male (% of male population)',\n",
    "'Population ages 60-64, female (% of female population)',\n",
    "'Population ages 60-64, male (% of male population)',\n",
    "'Population ages 65 and above (% of total population)',\n",
    "'Population ages 65 and above, female',\n",
    "'Population ages 65 and above, female (% of female population)',\n",
    "'Population ages 65 and above, male',\n",
    "'Population ages 65 and above, male (% of male population)',\n",
    "'Population ages 65 and above, total',\n",
    "'Population ages 65 and older (millions)',\n",
    "'Population ages 65-69, female (% of female population)',\n",
    "'Population ages 65-69, male (% of male population)',\n",
    "'Population ages 70-74, female (% of female population)',\n",
    "'Population ages 70-74, male (% of male population)',\n",
    "'Population ages 75-79, female (% of female population)',\n",
    "'Population ages 75-79, male (% of male population)',\n",
    "'Population ages 80 and above, female (% of female population)',\n",
    "'Population density (people per sq. km of land area)',\n",
    "'Population growth (annual %)',\n",
    "'Population under age 5 (millions)',\n",
    "'Population, female',\n",
    "'Population, male',\n",
    "'Population, total',\n",
    "'Renewable energy consumption (% of total final energy consumption)_y',\n",
    "'Revenue',\n",
    "'Population ages 05-09, male (% of male population)',\n",
    "'Population, female (% of total population)',\n",
    "'Population ages 20-24, male (% of male population)',\n",
    "'Rural population growth (annual %)',\n",
    "'Population ages 20-24, female (% of female population)',\n",
    "'Population ages 15-19, male (% of male population)',\n",
    "'Fertility rate, total (births per woman)',\n",
    "'Prevalence of anemia among children (% of children under 5)',\n",
    "'Agricultural raw materials imports (% of merchandise imports)',\n",
    "'Capture fisheries production (metric tons)',\n",
    "'Net imports [Imports - Exports - Bunkers] (petajoules)',\n",
    "'Prevalence of anemia among pregnant women (%)', \n",
    "'Employment in agriculture, male (% of male employment) (modeled ILO estimate)',\n",
    "'Immunization, measles (% of children ages 12-23 months)',\n",
    "'Prevalence of anemia among women of reproductive age (% of women ages 15-49)',\n",
    "'Prevalence of anemia among non-pregnant women (% of women ages 15-49)',\n",
    "'Young age (0-14) dependency ratio (per 100 people ages 15-64)',\n",
    "'Export unit value index (2000 = 100)',\n",
    "'Old-age (65 and older) dependency ratio (per 100 people ages 15-64)',\n",
    "'Infants lacking immunization, measles (% of one-year-olds)',\n",
    "'Gross national expenditure deflator (base year varies by country)',\n",
    "'General government final consumption expenditure (% of GDP)',\n",
    "'School enrollment, primary, female (% gross)',\n",
    "'Primary school starting age (years)',\n",
    "'GDP (constant 2010 US$)',\n",
    "'Inflation, average consumer prices',\n",
    "'Gross domestic savings (current US$)',   \n",
    "'Supply per capita (gigajoules)',\n",
    "'Primary income payments (BoP, current US$)',\n",
    "'Import volume index (2000 = 100)',\n",
    "'Emissions per capita (metric tons of carbon dioxide)',\n",
    "'Net primary income (BoP, current US$)',\n",
    "'Final consumption expenditure (% of GDP)',\n",
    "'Inflation, end of period consumer prices',\n",
    "'Gross capital formation (% of GDP)_x',\n",
    "'Gross capital formation (% of GDP)_y',\n",
    "'Gross capital formation (annual % growth)',\n",
    "'Gross capital formation (constant 2010 US$)',\n",
    "'Gross capital formation (current US$)',\n",
    "'Gross domestic product (GDP), total (2011 PPP $ billions)',\n",
    "'Gross domestic product based on purchasing-power-parity (PPP) share of world total',\n",
    "'Gross domestic product corresponding to fiscal year, current prices',\n",
    "'Gross domestic product per capita, constant prices',\n",
    "'Gross domestic product per capita, current prices',\n",
    "'Gross domestic product, constant prices',\n",
    "'Gross domestic product, current prices',\n",
    "'Gross domestic product, deflator',\n",
    "'Gross domestic savings (% of GDP)',\n",
    "'Gross domestic savings (current US$)',\n",
    "'Gross fixed capital formation (% of GDP)_x',\n",
    "'Gross fixed capital formation (% of GDP)_y',\n",
    "'Gross fixed capital formation (constant 2010 US$)',\n",
    "'Gross fixed capital formation (current US$)',\n",
    "'Gross intake ratio in first grade of primary education, total (% of relevant age group)',\n",
    "'Gross national expenditure (% of GDP)',\n",
    "'Gross national expenditure (constant 2010 US$)',\n",
    "'Gross national expenditure (current US$)',\n",
    "'Gross national expenditure deflator (base year varies by country)',\n",
    "'Gross national income (GNI) per capita (2011 PPP $)',\n",
    "'Gross savings (% of GDP)',\n",
    "'Gross savings (% of GNI)',\n",
    "'Gross savings (current US$)',\n",
    "'Households and NPISHs Final consumption expenditure (annual % growth)',\n",
    "'Households and NPISHs Final consumption expenditure (constant 2010 US$)',\n",
    "'Households and NPISHs Final consumption expenditure (current US$)',\n",
    "'Households and NPISHs Final consumption expenditure per capita (constant 2010 US$)',\n",
    "'Households and NPISHs Final consumption expenditure per capita growth (annual %)',\n",
    "'Households and NPISHs Final consumption expenditure, PPP (constant 2011 international $)',\n",
    "'Households and NPISHs final consumption expenditure (% of GDP)',\n",
    "'Domestic general government health expenditure (% of current health expenditure)',\n",
    "'Domestic general government health expenditure (% of general government expenditure)',\n",
    "'Domestic general government health expenditure per capita (current US$)',\n",
    "'Domestic general government health expenditure (% of GDP)',\n",
    "'Number of under-five deaths',\n",
    "'Overall loss in HDI due to inequality (%)',\n",
    "'Median age (years)',\n",
    "'Human Development Index (HDI), male',\n",
    "'Human Development Index (HDI), female',\n",
    "'Life expectancy at birth, total (years)',\n",
    "'Life expectancy at birth, male (years)',\n",
    "'Life expectancy at birth, female (years)',\n",
    "'Mean years of schooling, male (years)',\n",
    "'Mean years of schooling, female (years)',\n",
    "'Expected years of schooling, male (years)',\n",
    "'Expected years of schooling, female (years)',\n",
    "'Inequality-adjusted education index',\n",
    "'Inequality in life expectancy (%)',\n",
    "'Inequality-adjusted income index',\n",
    "'Inequality-adjusted HDI (IHDI)',\n",
    "'Inequality-adjusted life expectancy index',\n",
    "'Gender Inequality Index (GII)'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wellbeing Metric - Human Development Index (HDI)\n",
    "After evaluating a few different types of metrics, the Wellbeing metric I ended up using was the Human Development Index (HDI). It is not perfect but is pretty holistic / straight-forward and covers healthcare, education and income. For more details on how it is calculated, please go here: http://hdr.undp.org/sites/default/files/hdr2019_technical_notes.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the HDI construction\n",
    "life_expectancy = test['Life expectancy at birth (years)'].values[0]\n",
    "print(life_expectancy)\n",
    "expected_years = test['Expected years of schooling (years)'].values[0]\n",
    "print (expected_years)\n",
    "mean_years = test['Mean years of schooling (years)'].values[0]\n",
    "print (mean_years)\n",
    "gnipp = np.log(test['Gross national income (GNI) per capita (2011 PPP $)'].values[0])\n",
    "print (np.exp(gnipp))\n",
    "print (gnipp)\n",
    "\n",
    "# Creating a function to calculate the HDI based on inputs)\n",
    "\n",
    "def hdi_calculator (gnipp, life_expectancy, mean_years, expected_years):\n",
    "    health_index = (life_expectancy - 20) / (85 - 20)\n",
    "    expected_years_index = (expected_years - 0) / (18 - 0)\n",
    "    mean_years_index = (mean_years - 0) / (15 - 0)\n",
    "    education_index = (mean_years_index + expected_years_index) / 2\n",
    "    #income_index = (np.log(gnipp) - np.log (100)) / (np.log(75000) - np.log (100))\n",
    "    income_index = (gnipp - np.log (100)) / (np.log(75000) - np.log (100))\n",
    "    hdi = (health_index * education_index * income_index)**(1/3)\n",
    "    return hdi\n",
    "\n",
    "hdi_calculator(gnipp, life_expectancy, mean_years, expected_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a model to isolate top predictors\n",
    "\n",
    "# Creating the predictor matrix for train, dev and test sets\n",
    "X_train = train.drop(columns = predictors_to_drop + target_indicators_all + ['country_code' , 'country', 'date'])\n",
    "X_dev = dev.drop(columns = predictors_to_drop + target_indicators_all + ['country_code' , 'country', 'date'])\n",
    "X_test = test.drop(columns = predictors_to_drop + target_indicators_all + ['country_code' , 'country', 'date'])\n",
    "\n",
    "# For Timeseries Cross Validation. Since this data had time-bound data, this is an additional cross val test...\n",
    "# ...to ensure that I am not very off\n",
    "unique_years = work.date.unique()\n",
    "X = work.drop(columns = predictors_to_drop + target_indicators_all + ['country_code' , 'country'])\n",
    "\n",
    "# Initiating a StandardScaler object\n",
    "scaler_X = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Election\n",
    "At this point, I ran literally hundreds of different types of models trying to predict components of HDI (Mean Years of School, Expected Years of Schooling, GNIPP, Life Expectancy) as well trying to predict HDI as a final metric. \n",
    "\n",
    "Remember, in this step what was important was to able to isolate those indicators which were the strongest, least related and most controllable. It was important to be able to see feature importance / coefficient weights so I could only use model types that gave me those (Linear Regression, SVR Regression, Decision Tree Regressors including Boosting and Bagging Regressors). In the end, Linear Regression with Lasso regularlization was the most meaningful. That's the only one I show in this jupyter notebook for simplicitiy's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HDI Impact Checker - Lasso\n",
    "\n",
    "# Isolating the target variables\n",
    "target = 'Human Development Index (HDI)'\n",
    "\n",
    "y_train_hdi = train[target]\n",
    "y_dev_hdi = dev[target]\n",
    "y_test_hdi = test[target]\n",
    "y = work[['date'] + [target]]\n",
    "\n",
    "# Best model after Grid searching and evaluating many models\n",
    "model_hdi = LassoCV(alphas=np.logspace(-4, 4, 100), cv=5, fit_intercept=True, max_iter=10000, tol = 0.01)\n",
    "\n",
    "# Building a pipe to make the process more efficient\n",
    "pipe_hdi = Pipeline(steps=[\n",
    "                       ('standardize', scaler_X),\n",
    "                       ('model', model_hdi)])\n",
    "\n",
    "# Fitting the model\n",
    "pipe_hdi.fit (X_train,y_train_hdi)\n",
    "\n",
    "# Putting the predictions in a dataframe\n",
    "pred_hdi = pd.DataFrame(pipe_hdi.predict (X_dev))\n",
    "\n",
    "# Kfold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores_shuffled = cross_val_score(pipe_hdi, X_train, y_train_hdi, cv=kf)\n",
    "\n",
    "# Time_series cross validation (since this data had time-bound data, this is an additional cross val test to ensure...\n",
    "# ... that I am not very off.)\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "ts_scores_train = []\n",
    "ts_scores_test = []\n",
    "\n",
    "for train_index, test_index in tscv.split(unique_years):\n",
    "    X_train_ts = X[X.date.isin (unique_years[train_index])]\n",
    "    X_test_ts = X[X.date.isin (unique_years[test_index])]\n",
    "    X_train_ts.pop ('date')\n",
    "    X_test_ts.pop ('date')\n",
    "    \n",
    "    y_train_ts = y[y.date.isin (unique_years[train_index])]\n",
    "    y_test_ts = y[y.date.isin (unique_years[test_index])]\n",
    "    y_train_ts.pop ('date')\n",
    "    y_test_ts.pop ('date')\n",
    "    \n",
    "    ts_scores_train.append(pipe_hdi.score (X_train_ts, y_train_ts))\n",
    "    ts_scores_test.append(pipe_hdi.score (X_test_ts, y_test_ts))\n",
    "\n",
    "# Printing all ther results\n",
    "print(\"\\nKfold Cross Validation:\")\n",
    "print(\"Shuffled Cross-validated scores:\", scores_shuffled)\n",
    "print(\"Mean of Shuffled Cross-validated scores:\", scores_shuffled.mean())\n",
    "\n",
    "print(\"\\nTime Series Cross Validation:\")\n",
    "print(\"Train Time-Series Cross Val mean score:\", np.mean(ts_scores_train))\n",
    "print(\"Test Time-Series Cross Val mean score:\", np.mean(ts_scores_test))\n",
    "\n",
    "print(\"\\nTraining Score:\", pipe_hdi.score(X_train,y_train_hdi))\n",
    "print(\"Dev Score:\", pipe_hdi.score(X_dev,y_dev_hdi))\n",
    "print(\"Test Score:\", pipe_hdi.score(X_test,y_test_hdi))\n",
    "\n",
    "print (\"\\nRoot of Mean Squared Error:\", mean_squared_error (y_dev_hdi,pred_hdi)**0.5)\n",
    "\n",
    "print (\"\\nNegative Values:\", np.sum ((pred_hdi <= 0).values*1))\n",
    "\n",
    "# Building a dataframe for coefficients to isolate which ones are the most impactful\n",
    "df_coef = pd.DataFrame(list(zip(X_train.columns, pipe_hdi.named_steps['model'].coef_)))\n",
    "df_coef.columns = ['predictors', 'coefficients']\n",
    "df_coef['coef_abs'] = df_coef.coefficients.abs()\n",
    "df_coef.sort_values (by='coef_abs', ascending = False, inplace=True)\n",
    "df_coef[df_coef.coef_abs>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting these coefficients to a csv so that I can manually adjust and look for onces that are most impactful\n",
    "# Used MS Excel for this because it was a small dataset and more flexible for manual analysis\n",
    "df_coef.to_csv('../raw_data/picking_predictors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Using top selected indicators (19) to build best model to predict HDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the analysis done in Excel, these were top 19 indicators that were picked based on prediction power,...\n",
    "# ... controllability and relatedness (lack of)\n",
    "\n",
    "predictors_again = [\n",
    "'Inequality in education (%)',\n",
    "'School enrollment, primary (% gross)',\n",
    "'Domestic general government health expenditure per capita, PPP (current international $)',\n",
    "'Agriculture, forestry, and fishing, value added (% of GDP)',\n",
    "'Adolescent fertility rate (births per 1,000 women ages 15-19)',\n",
    "'Pupil-teacher ratio, primary',\n",
    "'Tuberculosis incidence (per 100,000 people)',\n",
    "'Secondary education, general pupils (% female)',\n",
    "'Out-of-pocket expenditure per capita, PPP (current international $)',\n",
    "'Inequality in income (%)',\n",
    "'PM2.5 pollution, population exposed to levels exceeding WHO Interim Target-3 value (% of total)',\n",
    "'Forest area (% of land area)',\n",
    "'Mobile cellular subscriptions (per 100 people)',\n",
    "'International inbound tourists (thousands)',\n",
    "'Government expenditure on education, total (% of GDP)',\n",
    "'Rural population with access to electricity (%)',\n",
    "'Unemployment, total (% of labour force)',\n",
    "'Corruption Perception Index',\n",
    "'Labor force participation rate, female (% of female population ages 15+) (modeled ILO estimate)',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the indicators, we have to rebuild the train, dev and test dataset\n",
    "# The reason to rebuild these datasets is because we have lesser indicators, we should have more rows to work with\n",
    "\n",
    "work_log = witer[['country_code', 'country', 'date'] + predictors_again + target_indicators_all].dropna()\n",
    "print (work_log.shape)\n",
    "work_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any negative values that might have been caused by imputation\n",
    "work_log['negative_vals'] = work_log[work_log[predictors_again] < 0].count(axis=1) >= 1\n",
    "work_log = work_log[work_log.negative_vals == False]\n",
    "work_log.pop('negative_vals')\n",
    "print (work_log.shape)\n",
    "work_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a deeper look at the indicators\n",
    "work_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing skew in the predictors so we can smooth the data as needed\n",
    "work_log[predictors_again].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging columns that have a skew greater than 1\n",
    "logged_columns = []\n",
    "for predictor in predictors_again:\n",
    "    if np.abs(work_log[predictor].skew()) >=1:\n",
    "        if np.abs(np.log(work_log[predictor]).skew()) < np.abs(work_log[predictor].skew()):\n",
    "            work_log[predictor] = np.log(work_log[predictor])\n",
    "            logged_columns.append(predictor)\n",
    "\n",
    "# As the output shows, the data is much less skewed after logging the relevant columns \n",
    "work_log[predictors_again].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the test / train / dev sets based on true values and iterated values as before\n",
    "true_values = woiter[work_log.columns].dropna()\n",
    "true_values = work_log[work_log.index.isin (true_values.index)]\n",
    "print (true_values.shape)\n",
    "true_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterated_values = work_log[~work_log.index.isin (true_values.index)]\n",
    "print (iterated_values.shape)\n",
    "iterated_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values.date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_true = true_values[true_values.date.isin (['2010','2012', '2014'])]\n",
    "print (dev_true.shape)\n",
    "dev_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterated_values.date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_iterated = iterated_values[iterated_values.date.isin (['2017', '2013', '2011'])]\n",
    "print (dev_iterated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_log = pd.concat([dev_true,dev_iterated])\n",
    "print (dev_log.shape)\n",
    "dev_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log = true_values[~true_values.index.isin (dev_true.index)]\n",
    "print (test_log.shape)\n",
    "test_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = work_log[~(work_log.index.isin (test_log.index)) & ~(work_log.index.isin (dev_log.index))]\n",
    "print (train_log.shape)\n",
    "train_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the train, dev and test sets are all aligned and there are no duplicates\n",
    "print (train_log.shape)\n",
    "print (dev_log.shape)\n",
    "print (test_log.shape)\n",
    "print (work_log.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating predictor matrices for all the train, dev and test datasets\n",
    "X_trimmed_train = train_log[predictors_again]\n",
    "X_trimmed_dev = dev_log[predictors_again]\n",
    "X_trimmed_test = test_log[predictors_again]\n",
    "X_trimmed = work_log[predictors_again + ['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring there are no null values\n",
    "X_trimmed_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection & Design:\n",
    "For the final algorithm structure, the idea was predict the components of HDI (Mean Years of School, Expected Years of Schooling, GNIPP, Life Expectancy), and then use those predictions to calculate HDI based on the formula as per the UNDP formulation (http://hdr.undp.org/sites/default/files/hdr2019_technical_notes.pdf).\n",
    "\n",
    "The goal was to eventually transfer this model to Tableau so I could create a toggle-able dashboard. Since I didnot have Tableau Server, I could not use TabPy and was only able to use Linear Regression. Why? Because Linear Regression gives me coefficients (and basically a formula) which I could then take to Tableau and use the model in the form of equations. \n",
    "\n",
    "Again, I toyed with many types of linear regression models and gridsearched all the four separate models of the four component indicators I was trying to predict to arrive at the best models which are presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for GNI PP - LASSO\n",
    "target = 'Gross national income (GNI) per capita (2011 PPP $)'\n",
    "\n",
    "y_train_gni = np.log(train_log[target])\n",
    "y_dev_gni = np.log(dev_log[target])\n",
    "y_test_gni = np.log(test_log[target])\n",
    "y = work_log[['date'] + [target]]\n",
    "y[target] = np.log(y[target].values)\n",
    "\n",
    "model_gni = LassoCV(alphas=np.logspace(-4, 4, 100), cv=5, eps=10, fit_intercept=True,\n",
    "                         max_iter=100, n_alphas=10, n_jobs=None,\n",
    "                         normalize=False, positive=False, precompute='auto',\n",
    "                         random_state=None, selection='cyclic', tol=0.001,\n",
    "                         verbose=False)\n",
    "\n",
    "pipe_gni = Pipeline(steps=[\n",
    "                       ('standardize', scaler_X),\n",
    "                       ('model', model_gni)])\n",
    "\n",
    "pipe_gni.fit (X_trimmed_train,y_train_gni)\n",
    "\n",
    "pred_gni = pd.DataFrame(pipe_gni.predict (X_trimmed_dev))\n",
    "\n",
    "#kfold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores_shuffled = cross_val_score(pipe_gni, X_trimmed_train, y_train_gni, cv=kf)\n",
    "\n",
    "#time_series cross validation\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "ts_scores_train = []\n",
    "ts_scores_test = []\n",
    "\n",
    "for train_index, test_index in tscv.split(unique_years):\n",
    "    X_trimmed_train_ts = X_trimmed[X_trimmed.date.isin (unique_years[train_index])]\n",
    "    X_trimmed_test_ts = X_trimmed[X_trimmed.date.isin (unique_years[test_index])]\n",
    "    X_trimmed_train_ts.pop ('date')\n",
    "    X_trimmed_test_ts.pop ('date')\n",
    "    \n",
    "    y_train_ts = y[y.date.isin (unique_years[train_index])]\n",
    "    y_test_ts = y[y.date.isin (unique_years[test_index])]\n",
    "    y_train_ts.pop ('date')\n",
    "    y_test_ts.pop ('date')\n",
    "    \n",
    "    ts_scores_train.append(pipe_gni.score (X_trimmed_train_ts, y_train_ts))\n",
    "    ts_scores_test.append(pipe_gni.score (X_trimmed_test_ts, y_test_ts))\n",
    "\n",
    "print(\"\\nKfold Cross Validation:\")\n",
    "print(\"Shuffled Cross-validated scores:\", scores_shuffled)\n",
    "print(\"Mean of Shuffled Cross-validated scores:\", scores_shuffled.mean())\n",
    "\n",
    "print(\"\\nTime Series Cross Validation:\")\n",
    "print(\"Train Time-Series Cross Val mean score:\", np.mean(ts_scores_train))\n",
    "print(\"Test Time-Series Cross Val mean score:\", np.mean(ts_scores_test))\n",
    "\n",
    "print(\"\\nTraining Score:\", pipe_gni.score(X_trimmed_train,y_train_gni))\n",
    "print(\"Dev Score:\", pipe_gni.score(X_trimmed_dev,y_dev_gni))\n",
    "print(\"Test Score:\", pipe_gni.score(X_trimmed_test,y_test_gni))\n",
    "\n",
    "print (\"\\nRoot of Mean Squared Error:\", mean_squared_error (y_dev_gni,pred_gni)**0.5)\n",
    "\n",
    "print (\"\\nNegative Values:\", np.sum ((pred_gni <= 0).values*1))\n",
    "\n",
    "# Building a dataframe for coefficients\n",
    "df_coef = pd.DataFrame(list(zip(X_trimmed_train.columns, pipe_gni.named_steps['model'].coef_)))\n",
    "df_coef.columns = ['predictors', 'coefficients']\n",
    "df_coef['coef_abs'] = df_coef.coefficients.abs()\n",
    "df_coef.sort_values (by='coef_abs', ascending = False, inplace=True)\n",
    "df_coef[df_coef.coef_abs>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Life Expectancy - LASSO\n",
    "target = 'Life expectancy at birth (years)'\n",
    "\n",
    "y_train_le = train_log[target]\n",
    "y_dev_le = dev_log[target]\n",
    "y_test_le = test_log[target]\n",
    "y = work_log[['date'] + [target]]\n",
    "\n",
    "model_le = LassoCV(alphas=np.logspace(-4, 4, 100), cv=5, fit_intercept=True, max_iter=10000, tol = 0.01)\n",
    "\n",
    "pipe_le = Pipeline(steps=[\n",
    "                       ('standardize', scaler_X),\n",
    "                       ('model', model_le)])\n",
    "\n",
    "pipe_le.fit (X_trimmed_train,y_train_le)\n",
    "\n",
    "pred_le = pd.DataFrame(pipe_le.predict (X_trimmed_dev))\n",
    "\n",
    "#kfold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores_shuffled = cross_val_score(pipe_le, X_trimmed_train, y_train_le, cv=kf)\n",
    "\n",
    "#time_series cross validation\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "ts_scores_train = []\n",
    "ts_scores_test = []\n",
    "\n",
    "for train_index, test_index in tscv.split(unique_years):\n",
    "    X_trimmed_train_ts = X_trimmed[X_trimmed.date.isin (unique_years[train_index])]\n",
    "    X_trimmed_test_ts = X_trimmed[X_trimmed.date.isin (unique_years[test_index])]\n",
    "    X_trimmed_train_ts.pop ('date')\n",
    "    X_trimmed_test_ts.pop ('date')\n",
    "    \n",
    "    y_train_ts = y[y.date.isin (unique_years[train_index])]\n",
    "    y_test_ts = y[y.date.isin (unique_years[test_index])]\n",
    "    y_train_ts.pop ('date')\n",
    "    y_test_ts.pop ('date')\n",
    "    \n",
    "    ts_scores_train.append(pipe_le.score (X_trimmed_train_ts, y_train_ts))\n",
    "    ts_scores_test.append(pipe_le.score (X_trimmed_test_ts, y_test_ts))\n",
    "\n",
    "print(\"\\nKfold Cross Validation:\")\n",
    "print(\"Shuffled Cross-validated scores:\", scores_shuffled)\n",
    "print(\"Mean of Shuffled Cross-validated scores:\", scores_shuffled.mean())\n",
    "\n",
    "print(\"\\nTime Series Cross Validation:\")\n",
    "print(\"Train Time-Series Cross Val mean score:\", np.mean(ts_scores_train))\n",
    "print(\"Test Time-Series Cross Val mean score:\", np.mean(ts_scores_test))\n",
    "\n",
    "print(\"\\nTraining Score:\", pipe_le.score(X_trimmed_train,y_train_le))\n",
    "print(\"Dev Score:\", pipe_le.score(X_trimmed_dev,y_dev_le))\n",
    "print(\"Test Score:\", pipe_le.score(X_trimmed_test,y_test_le))\n",
    "\n",
    "print (\"\\nRoot of Mean Squared Error:\", mean_squared_error (y_dev_le,pred_le)**0.5)\n",
    "\n",
    "print (\"\\nNegative Values:\", np.sum ((pred_le <= 0).values*1))\n",
    "\n",
    "# Building a dataframe for coefficients\n",
    "df_coef = pd.DataFrame(list(zip(X_trimmed_train.columns, pipe_le.named_steps['model'].coef_)))\n",
    "df_coef.columns = ['predictors', 'coefficients']\n",
    "df_coef['coef_abs'] = df_coef.coefficients.abs()\n",
    "df_coef.sort_values (by='coef_abs', ascending = False, inplace=True)\n",
    "df_coef[df_coef.coef_abs>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Mean years of schooling - LASSO\n",
    "target = 'Mean years of schooling (years)'\n",
    "\n",
    "y_train_mys = train_log[target]\n",
    "y_dev_mys = dev_log[target]\n",
    "y_test_mys = test_log[target]\n",
    "y = work_log[['date'] + [target]]\n",
    "\n",
    "model_mys = LassoCV(alphas=np.logspace(-4, 4, 100), cv=5, fit_intercept=True, max_iter=10000, tol = 0.01)\n",
    "\n",
    "pipe_mys = Pipeline(steps=[\n",
    "                       ('standardize', scaler_X),\n",
    "                       ('model', model_mys)])\n",
    "\n",
    "pipe_mys.fit (X_trimmed_train,y_train_mys)\n",
    "\n",
    "pred_mys = pd.DataFrame(pipe_mys.predict (X_trimmed_dev))\n",
    "\n",
    "#kfold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores_shuffled = cross_val_score(pipe_mys, X_trimmed_train, y_train_mys, cv=kf)\n",
    "\n",
    "#time_series cross validation\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "ts_scores_train = []\n",
    "ts_scores_test = []\n",
    "\n",
    "for train_index, test_index in tscv.split(unique_years):\n",
    "    X_trimmed_train_ts = X_trimmed[X_trimmed.date.isin (unique_years[train_index])]\n",
    "    X_trimmed_test_ts = X_trimmed[X_trimmed.date.isin (unique_years[test_index])]\n",
    "    X_trimmed_train_ts.pop ('date')\n",
    "    X_trimmed_test_ts.pop ('date')\n",
    "    \n",
    "    y_train_ts = y[y.date.isin (unique_years[train_index])]\n",
    "    y_test_ts = y[y.date.isin (unique_years[test_index])]\n",
    "    y_train_ts.pop ('date')\n",
    "    y_test_ts.pop ('date')\n",
    "    \n",
    "    ts_scores_train.append(pipe_mys.score (X_trimmed_train_ts, y_train_ts))\n",
    "    ts_scores_test.append(pipe_mys.score (X_trimmed_test_ts, y_test_ts))\n",
    "\n",
    "print(\"\\nKfold Cross Validation:\")\n",
    "print(\"Shuffled Cross-validated scores:\", scores_shuffled)\n",
    "print(\"Mean of Shuffled Cross-validated scores:\", scores_shuffled.mean())\n",
    "\n",
    "print(\"\\nTime Series Cross Validation:\")\n",
    "print(\"Train Time-Series Cross Val mean score:\", np.mean(ts_scores_train))\n",
    "print(\"Test Time-Series Cross Val mean score:\", np.mean(ts_scores_test))\n",
    "\n",
    "print(\"\\nTraining Score:\", pipe_mys.score(X_trimmed_train,y_train_mys))\n",
    "print(\"Dev Score:\", pipe_mys.score(X_trimmed_dev,y_dev_mys))\n",
    "print(\"Test Score:\", pipe_mys.score(X_trimmed_test,y_test_mys))\n",
    "\n",
    "print (\"\\nRoot of Mean Squared Error:\", mean_squared_error (y_dev_mys,pred_mys)**0.5)\n",
    "\n",
    "print (\"\\nNegative Values:\", np.sum ((pred_mys <= 0).values*1))\n",
    "\n",
    "# Building a dataframe for coefficients\n",
    "df_coef = pd.DataFrame(list(zip(X_trimmed_train.columns, pipe_mys.named_steps['model'].coef_)))\n",
    "df_coef.columns = ['predictors', 'coefficients']\n",
    "df_coef['coef_abs'] = df_coef.coefficients.abs()\n",
    "df_coef.sort_values (by='coef_abs', ascending = False, inplace=True)\n",
    "df_coef[df_coef.coef_abs>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Expected years of schooling - LASSO\n",
    "target = 'Expected years of schooling (years)'\n",
    "\n",
    "y_train_eys = train_log[target]\n",
    "y_dev_eys = dev_log[target]\n",
    "y_test_eys = test_log[target]\n",
    "y = work_log[['date'] + [target]]\n",
    "\n",
    "model_eys = LassoCV(alphas=np.logspace(-4, 4, 100), cv=5, fit_intercept=True, max_iter=10000, tol = 0.01)\n",
    "\n",
    "pipe_eys = Pipeline(steps=[\n",
    "                       ('standardize', scaler_X),\n",
    "                       ('model', model_eys)])\n",
    "\n",
    "pipe_eys.fit (X_trimmed_train,y_train_eys)\n",
    "\n",
    "pred_eys = pd.DataFrame(pipe_eys.predict (X_trimmed_dev))\n",
    "\n",
    "#kfold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores_shuffled = cross_val_score(pipe_eys, X_trimmed_train, y_train_eys, cv=kf)\n",
    "\n",
    "#time_series cross validation\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "ts_scores_train = []\n",
    "ts_scores_test = []\n",
    "\n",
    "for train_index, test_index in tscv.split(unique_years):\n",
    "    X_trimmed_train_ts = X_trimmed[X_trimmed.date.isin (unique_years[train_index])]\n",
    "    X_trimmed_test_ts = X_trimmed[X_trimmed.date.isin (unique_years[test_index])]\n",
    "    X_trimmed_train_ts.pop ('date')\n",
    "    X_trimmed_test_ts.pop ('date')\n",
    "    \n",
    "    y_train_ts = y[y.date.isin (unique_years[train_index])]\n",
    "    y_test_ts = y[y.date.isin (unique_years[test_index])]\n",
    "    y_train_ts.pop ('date')\n",
    "    y_test_ts.pop ('date')\n",
    "    \n",
    "    ts_scores_train.append(pipe_eys.score (X_trimmed_train_ts, y_train_ts))\n",
    "    ts_scores_test.append(pipe_eys.score (X_trimmed_test_ts, y_test_ts))\n",
    "\n",
    "print(\"\\nKfold Cross Validation:\")\n",
    "print(\"Shuffled Cross-validated scores:\", scores_shuffled)\n",
    "print(\"Mean of Shuffled Cross-validated scores:\", scores_shuffled.mean())\n",
    "\n",
    "print(\"\\nTime Series Cross Validation:\")\n",
    "print(\"Train Time-Series Cross Val mean score:\", np.mean(ts_scores_train))\n",
    "print(\"Test Time-Series Cross Val mean score:\", np.mean(ts_scores_test))\n",
    "\n",
    "print(\"\\nTraining Score:\", pipe_eys.score(X_trimmed_train,y_train_eys))\n",
    "print(\"Dev Score:\", pipe_eys.score(X_trimmed_dev,y_dev_eys))\n",
    "print(\"Test Score:\", pipe_eys.score(X_trimmed_test,y_test_eys))\n",
    "\n",
    "print (\"\\nRoot of Mean Squared Error:\", mean_squared_error (y_dev_eys,pred_eys)**0.5)\n",
    "\n",
    "print (\"\\nNegative Values:\", np.sum ((pred_eys <= 0).values*1))\n",
    "\n",
    "# Building a dataframe for coefficients\n",
    "df_coef = pd.DataFrame(list(zip(X_trimmed_train.columns, pipe_eys.named_steps['model'].coef_)))\n",
    "df_coef.columns = ['predictors', 'coefficients']\n",
    "df_coef['coef_abs'] = df_coef.coefficients.abs()\n",
    "df_coef.sort_values (by='coef_abs', ascending = False, inplace=True)\n",
    "df_coef[df_coef.coef_abs>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing final models on how they predict HDI versus actual HDI\n",
    "gni_predictions = pipe_gni.predict (X_trimmed_test)\n",
    "le_predictions = pipe_le.predict (X_trimmed_test)\n",
    "mys_predictions = pipe_mys.predict (X_trimmed_test)\n",
    "eys_predictions = pipe_eys.predict (X_trimmed_test)\n",
    "\n",
    "hdi_predicted = []\n",
    "for i in range(len(gni_predictions)):\n",
    "    hdi = hdi_calculator (gni_predictions[i],le_predictions[i],mys_predictions[i],eys_predictions[i])\n",
    "    hdi_predicted.append(hdi)\n",
    "hdi_predicted = np.nan_to_num (hdi_predicted)\n",
    "\n",
    "hdi_test = test_log['Human Development Index (HDI)']\n",
    "\n",
    "# R2 score for predicted HDIs based on our models and actual HDIs\n",
    "r2_score(hdi_test, hdi_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing final models on how they predict HDI versus actual HDI only for true values\n",
    "test_clean = woiter[['country_code','country', 'date'] + predictors_again + target_indicators_all].dropna()\n",
    "for col in logged_columns:\n",
    "    test_clean[col] = np.log(test_clean[col])\n",
    "\n",
    "print(test_clean.shape)\n",
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_clean = test_clean[predictors_again]\n",
    "print (X_test_clean.shape)\n",
    "X_test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing final models on how they predict HDI versus actual HDI only for true values\n",
    "gni_predictions = pipe_gni.predict (X_test_clean)\n",
    "le_predictions = pipe_le.predict (X_test_clean)\n",
    "mys_predictions = pipe_mys.predict (X_test_clean)\n",
    "eys_predictions = pipe_eys.predict (X_test_clean)\n",
    "\n",
    "hdi_predicted = []\n",
    "for i in range(len(gni_predictions)):\n",
    "    hdi = hdi_calculator (gni_predictions[i],le_predictions[i],mys_predictions[i],eys_predictions[i])\n",
    "    hdi_predicted.append(hdi)\n",
    "hdi_predicted = np.nan_to_num (hdi_predicted)\n",
    "\n",
    "hdi_test = test_clean['Human Development Index (HDI)']\n",
    "\n",
    "# R2 score for predicted HDIs based on our models and actual HDIs only for true values (excluding iterated values)\n",
    "r2_score(hdi_test, hdi_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Exporting model to Tableau through CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the models are built, we need to export the data and the model coefficients into CSVs to take to Tableau\n",
    "\n",
    "# Building a function to create coefficient tables for export\n",
    "def coef_table_creator_lasso (fitted_model, predictor_df, name_str):\n",
    "    coef_tbl = pd.DataFrame(list(zip((name_str+predictor_df.columns), fitted_model.coef_)), columns = ['Constant', 'Value'])\n",
    "    intercept_tbl = pd.DataFrame({'Constant' : [name_str+'Intercept'], \n",
    "               'Value' : [fitted_model.intercept_]})\n",
    "    final_table = pd.concat ([coef_tbl, intercept_tbl])\n",
    "    return final_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gni_tbl = coef_table_creator_lasso (pipe_gni.named_steps['model'], X_trimmed_train, 'GNI_')\n",
    "le_tbl = coef_table_creator_lasso (pipe_le.named_steps['model'], X_trimmed_train, 'LE_')\n",
    "mys_tbl = coef_table_creator_lasso (pipe_mys.named_steps['model'],  X_trimmed_train, 'MYS_')\n",
    "eys_tbl = coef_table_creator_lasso (pipe_eys.named_steps['model'],  X_trimmed_train, 'EYS_')\n",
    "std_tbl = pd.DataFrame(list(zip(('STD_'+ X_trimmed_train.columns), scaler_X.scale_)), columns = ['Constant', 'Value'])\n",
    "mn_tbl = pd.DataFrame(list(zip(('MN_'+ X_trimmed_train.columns), scaler_X.mean_)), columns = ['Constant', 'Value'])\n",
    "\n",
    "# Merging all the tables into one final table for export\n",
    "model_table_one = pd.concat([gni_tbl, le_tbl,mys_tbl,eys_tbl, std_tbl, mn_tbl]).reset_index(drop=True)\n",
    "model_table_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting final model table for Tableau\n",
    "model_table_one.index = model_table_one.Constant\n",
    "model_table_one.pop('Constant')\n",
    "model_table_one.T.to_csv ('../tableau_data/model_table_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing clean (true values) data for Tableau export\n",
    "data_clean = woiter[['country_code', 'country', 'date'] + predictors_again + target_indicators_all + ['High Inc Low Inc Class']]\n",
    "print(data_clean.shape)\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing iterated data for Tableau export\n",
    "data_iterated = witer[['country_code', 'country', 'date'] + predictors_again + target_indicators_all + ['High Inc Low Inc Class']]\n",
    "data_iterated['negative_vals'] = data_iterated[data_iterated[predictors_again + target_indicators_all] < 0].count(axis=1) >= 1\n",
    "data_iterated = data_iterated[data_iterated.negative_vals == False]\n",
    "data_iterated.pop('negative_vals')\n",
    "print(data_iterated.shape)\n",
    "data_iterated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing logged data for Tableau export\n",
    "data_log = work_log.copy()\n",
    "print (data_log.shape)\n",
    "data_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting data for Tableau\n",
    "data_clean.to_csv ('../tableau_data/data_clean.csv', index=False)\n",
    "data_iterated.to_csv ('../tableau_data/data_iterated.csv', index=False)\n",
    "data_log.to_csv ('../tableau_data/data_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Model to generate feature importance for Tableau\n",
    "target = 'Human Development Index (HDI)'\n",
    "\n",
    "y_train_hdi_imp = train_log[target]\n",
    "y_dev_hdi_imp = dev_log[target]\n",
    "y_test_hdi_imp = test_log[target]\n",
    "y = work_log[['date'] + [target]]\n",
    "\n",
    "model_hdi_imp = LassoCV(alphas=np.logspace(-4, 4, 100), cv=5, fit_intercept=True, max_iter=10000, tol = 0.01)\n",
    "\n",
    "pipe_hdi_imp = Pipeline(steps=[\n",
    "                       ('standardize', scaler_X),\n",
    "                       ('model', model_hdi_imp)])\n",
    "\n",
    "pipe_hdi_imp.fit (X_trimmed_train,y_train_hdi_imp)\n",
    "\n",
    "pred_hdi_imp = pd.DataFrame(pipe_hdi_imp.predict (X_trimmed_dev))\n",
    "\n",
    "#kfold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores_shuffled = cross_val_score(pipe_hdi_imp, X_trimmed_train, y_train_hdi_imp, cv=kf)\n",
    "\n",
    "#time_series cross validation\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "ts_scores_train = []\n",
    "ts_scores_test = []\n",
    "\n",
    "for train_index, test_index in tscv.split(unique_years):\n",
    "    X_trimmed_train_ts = X_trimmed[X_trimmed.date.isin (unique_years[train_index])]\n",
    "    X_trimmed_test_ts = X_trimmed[X_trimmed.date.isin (unique_years[test_index])]\n",
    "    X_trimmed_train_ts.pop ('date')\n",
    "    X_trimmed_test_ts.pop ('date')\n",
    "    \n",
    "    y_train_ts = y[y.date.isin (unique_years[train_index])]\n",
    "    y_test_ts = y[y.date.isin (unique_years[test_index])]\n",
    "    y_train_ts.pop ('date')\n",
    "    y_test_ts.pop ('date')\n",
    "    \n",
    "    ts_scores_train.append(pipe_hdi_imp.score (X_trimmed_train_ts, y_train_ts))\n",
    "    ts_scores_test.append(pipe_hdi_imp.score (X_trimmed_test_ts, y_test_ts))\n",
    "\n",
    "print(\"\\nKfold Cross Validation:\")\n",
    "print(\"Shuffled Cross-validated scores:\", scores_shuffled)\n",
    "print(\"Mean of Shuffled Cross-validated scores:\", scores_shuffled.mean())\n",
    "\n",
    "print(\"\\nTime Series Cross Validation:\")\n",
    "print(\"Train Time-Series Cross Val mean score:\", np.mean(ts_scores_train))\n",
    "print(\"Test Time-Series Cross Val mean score:\", np.mean(ts_scores_test))\n",
    "\n",
    "print(\"\\nTraining Score:\", pipe_hdi_imp.score(X_trimmed_train,y_train_hdi_imp))\n",
    "print(\"Dev Score:\", pipe_hdi_imp.score(X_trimmed_dev,y_dev_hdi_imp))\n",
    "print(\"Test Score:\", pipe_hdi_imp.score(X_trimmed_test,y_test_hdi_imp))\n",
    "\n",
    "print (\"\\nRoot of Mean Squared Error:\", mean_squared_error (y_dev_hdi_imp,pred_hdi_imp)**0.5)\n",
    "\n",
    "print (\"\\nNegative Values:\", np.sum ((pred_hdi_imp <= 0).values*1))\n",
    "\n",
    "# Building a dataframe for coefficients\n",
    "df_coef = pd.DataFrame(list(zip(X_trimmed_train.columns, pipe_hdi_imp.named_steps['model'].coef_)))\n",
    "df_coef.columns = ['predictors', 'coefficients']\n",
    "df_coef['coef_abs'] = df_coef.coefficients.abs()\n",
    "df_coef.sort_values (by='coef_abs', ascending = False, inplace=True)\n",
    "df_coef[df_coef.coef_abs>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a relative_imp column to calculate relative importance to be used in Tableau\n",
    "df_coef['relative_imp'] = df_coef.coef_abs.apply(lambda x: x/(np.sum(df_coef.coef_abs)))\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting feature importance file for Tableau usage\n",
    "df_coef.to_csv ('../tableau_data/feature_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for Tableau!\n",
    "\n",
    "The final step of this project is to take the model and the data into Tableau to build an interactive dashboard that can actually be used by non-coding users to potentially make better hypothesis on what impacts well-being and how."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
